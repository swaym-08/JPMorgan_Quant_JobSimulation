# -*- coding: utf-8 -*-
"""JPMorgoan_Quant2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aaWXohCRd4X3AmaDMapWAXUB1z3i3NNi

#Task 3

#Import dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

"""#Using SVM"""

loan_df=pd.read_csv('/content/drive/MyDrive/Quantitative Analysis/Task 3 and 4_Loan_Data.csv')
loan_df.head()

X=loan_df.drop(['default','customer_id'],axis=1)
y=loan_df['default']

scaler=StandardScaler()
X=scaler.fit_transform(X)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)

svc=SVC(probability=True)
svm=svc.fit(X_train,y_train)

y_pred=svm.predict(X_test)

conf_matrix=confusion_matrix(y_test,y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

svm.score(X_test,y_test)*100

"""#Using RandomForestClassifier"""

rfc=RandomForestClassifier(n_estimators=50)
rfc.fit(X_train, y_train)

y_pred=rfc.predict(X_test)

conf_matrix=confusion_matrix(y_test,y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

rfc.score(X_test,y_test)*100

feature_names=loan_df.drop(['default','customer_id'],axis=1).columns
feature_importance=[round(x,5) for x in rfc.feature_importances_]

plt.figure(figsize=(10,6))
sns.barplot(x=feature_importance,y=feature_names)
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance')
plt.show()

"""#Expected Loss"""

def calculate_expected_loss(model,scaler,features,recovery_rate=0.1):
  features_scaled=scaler.transform([features])

  probability_of_default=model.predict_proba(features_scaled)[:,1][0]

  #Loss Given Default
  LGD=1-recovery_rate

  #Exposure At Default
  EAD=features[1]

  expected_loss=probability_of_default*LGD*EAD

  return expected_loss

import warnings
warnings.filterwarnings('ignore')

borrower=[5,1958.928726,8228.752520,26648.43525,2,572]

expected_loss=calculate_expected_loss(svm,scaler,borrower)
print(f'Expected Loss acc to SVM: ${expected_loss:.2f}')

expected_loss=calculate_expected_loss(rfc,scaler,borrower)
print(f'Expected Loss acc to RandomForestClassifier: ${expected_loss:.2f}')

"""#Task 4

#Setting up Buckets
"""

num_buckets=4
loan_df['bucket']=pd.cut(loan_df['fico_score'],bins=num_buckets,labels=False)

loan_df

from scipy.stats import binom
def calculate_log_likelihood(data):
  log_likelihood=0;
  for b in range(num_buckets):
    bucket_data=data[data['bucket']==b]
    if not bucket_data.empty:
      ni=len(bucket_data)
      ki=bucket_data['default'].sum()
      pi=ki/ni if ni>0 else 0
      log_likelihood+=binom.logpmf(ki,ni,pi)
  return log_likelihood

initial_log_likelihood=calculate_log_likelihood(loan_df)

plt.figure(figsize=(10,6))
for b in range(num_buckets):
  bucket_data=loan_df[loan_df['bucket']==b]
  plt.scatter(bucket_data['fico_score'],bucket_data['default'],marker='o',label=f'Bucket {b}')

plt.xlabel('Bucket')
plt.ylabel('FICO Score')
plt.legend()
plt.show()

"""#Bucket Optimization"""

def optimize_buckets(data,num_buckets,max_iterations=100):
  boundaries=np.linspace(data['fico_score'].min(),data['fico_score'].max(),num_buckets+1)
  best_log_likelihood=calculate_log_likelihood(data)
  improved=True
  iteration=0

  while improved and iteration<max_iterations:
    improved=False
    for i in range(len(boundaries)-1):
      original_boundary=boundaries[i]
      for shift in [-10,10]:
        boundaries[i]+=shift
        data['bucket']=pd.cut(data['fico_score'],bins=boundaries,labels=False)
        new_log_likelihood=calculate_log_likelihood(data)

        if new_log_likelihood>best_log_likelihood:
          best_log_likelihood=new_log_likelihood
          improved=True
        else:
          boundaries[i]=original_boundary
    iteration+=1
    # print(f"Iteration {iteration},log-likelihood: {best_log_likelihood}")

  return boundaries

final_boundaries=optimize_buckets(loan_df,num_buckets)

loan_df['bucket']=pd.cut(loan_df['fico_score'],bins=final_boundaries,labels=False)

final_boundaries

"""#Step forward-> Number of buckets optimization"""

def optimize_bucket_count(data,min_buckets,max_buckets):
  best_bucket_count=min_buckets
  best_log_likelihood=-np.inf
  best_boundaries=None

  for num_buckets in range(min_buckets,max_buckets):

    boundaries=np.linspace(data['fico_score'].min(),data['fico_score'].max(),num_buckets+1)
    data['bucket']=pd.cut(data['fico_score'],bins=boundaries,labels=False)

    boundaries=optimize_buckets(data,num_buckets)
    data['bucket']=pd.cut(data['fico_score'],bins=boundaries,labels=False)
    log_likelihood=calculate_log_likelihood(data)
    print(f"Number of buckets: {num_buckets},log-likelihood: {log_likelihood}")

    if log_likelihood>best_log_likelihood:
      best_log_likelihood=log_likelihood
      best_bucket_count=num_buckets
      best_boundaries=boundaries

  return best_bucket_count,best_boundaries,best_log_likelihood

best_bucket_count,final_boundaries,best_log_likelihood=optimize_bucket_count(loan_df,3,11)
print(f"Optimal number of buckets: {best_bucket_count}")
print(f"Optimal boundaries: {final_boundaries}")
print(f"Optimal log-likelihood: {best_log_likelihood}")